{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13710936,"sourceType":"datasetVersion","datasetId":8722438},{"sourceId":13711367,"sourceType":"datasetVersion","datasetId":8722723},{"sourceId":13715694,"sourceType":"datasetVersion","datasetId":8725719}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nimport torch","metadata":{"id":"wCPAKiS_ZD4V","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:19:13.872644Z","iopub.execute_input":"2025-12-19T10:19:13.872796Z","iopub.status.idle":"2025-12-19T10:19:22.315964Z","shell.execute_reply.started":"2025-12-19T10:19:13.872781Z","shell.execute_reply":"2025-12-19T10:19:22.315108Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os  \nimport pandas as pd  \nimport spacy \nimport torch\nfrom torch.nn.utils.rnn import pad_sequence \nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  \nimport torchvision.transforms as transforms\n\n\n\nspacy_eng = spacy.load(\"en_core_web_sm\")\n\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n\nclass FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n\n        \n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n        \n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n\n        return imgs, targets\n\n\ndef get_loader(\n    root_folder,\n    annotation_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n):\n    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset\n\n\nif __name__ == \"__main__\":\n    transform = transforms.Compose(\n        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n    )\n\n    loader, dataset = get_loader(\n        \"/kaggle/input/flikr8k/Images\", \"/kaggle/input/flikr8k/captions.txt\", transform=transform\n    )\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbN5TmCPYy1Y","outputId":"fec8d573-2518-4fbe-98fa-dbea94ec3acd","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:19:22.318041Z","iopub.execute_input":"2025-12-19T10:19:22.318365Z","iopub.status.idle":"2025-12-19T10:19:29.566781Z","shell.execute_reply.started":"2025-12-19T10:19:22.318349Z","shell.execute_reply":"2025-12-19T10:19:29.565964Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch.nn as nn\nimport statistics\nimport torchvision.models as models\nfrom torchvision.models import inception_v3, Inception_V3_Weights\n\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size, train_CNN=False):\n        super(EncoderCNN, self).__init__()\n        self.train_CNN = train_CNN\n        self.inception = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n        self.inception.aux_logits = False \n        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n        self.relu = nn.ReLU()\n        self.times = []\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, images):\n        features = self.inception(images)\n        return self.dropout(self.relu(features))\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, features, captions):\n        embeddings = self.dropout(self.embed(captions))\n        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n        hiddens, _ = self.lstm(embeddings)\n        outputs = self.linear(hiddens)\n        return outputs\n\n\nclass CNNtoRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n        super(CNNtoRNN, self).__init__()\n        self.encoderCNN = EncoderCNN(embed_size)\n        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n\n    def forward(self, images, captions):\n        features = self.encoderCNN(images)\n        outputs = self.decoderRNN(features, captions)\n        return outputs\n\n    def caption_image(self, image, vocabulary, max_length=50):\n        result_caption = []\n\n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n\n            for _ in range(max_length):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n\n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n\n        return [vocabulary.itos[idx] for idx in result_caption]\n","metadata":{"id":"KNlBWr25adAb","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:19:29.567607Z","iopub.execute_input":"2025-12-19T10:19:29.567800Z","iopub.status.idle":"2025-12-19T10:19:29.582921Z","shell.execute_reply.started":"2025-12-19T10:19:29.567785Z","shell.execute_reply":"2025-12-19T10:19:29.582414Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"id":"-Bo-9owUf2wZ","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:19:29.583668Z","iopub.execute_input":"2025-12-19T10:19:29.583927Z","iopub.status.idle":"2025-12-19T10:19:29.598624Z","shell.execute_reply.started":"2025-12-19T10:19:29.583905Z","shell.execute_reply":"2025-12-19T10:19:29.598002Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef train():\n    \n    transform = transforms.Compose(\n        [\n            transforms.Resize((356, 356)),\n            transforms.RandomCrop((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    \n    train_loader, dataset = get_loader(\n        root_folder=\"/kaggle/input/flikr8k/Images\",\n        annotation_file=\"/kaggle/input/flikr8k/captions.txt\",\n        transform=transform,\n        num_workers=2,\n    )\n\n    \n    torch.backends.cudnn.benchmark = True\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    load_model = False\n    save_model = True\n    train_CNN = False\n\n   \n    embed_size = 256\n    hidden_size = 256\n    vocab_size = len(dataset.vocab)\n    num_layers = 1\n    learning_rate = 3e-4\n    num_epochs = 50  \n\n   \n    writer = SummaryWriter(\"/kaggle/working/runs/flikr\")\n    step = 0\n\n    \n    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    \n    for name, param in model.encoderCNN.inception.named_parameters():\n        if \"fc.weight\" in name or \"fc.bias\" in name:\n            param.requires_grad = True\n        else:\n            param.requires_grad = train_CNN\n\n   \n    if load_model:\n        step = load_checkpoint(torch.load(\"/kaggle/working/my_checkpoint.pth.tar\"), model, optimizer)\n\n    model.train()\n    print(f\"Training started on device: {device}\")\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] -----------------------------\")\n        running_loss = 0.0\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(train_loader), total=len(train_loader), leave=False\n        ):\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs = model(imgs, captions[:-1])\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n\n            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n            step += 1\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Print progress\n            if idx % 100 == 0:\n                avg_loss = running_loss / (idx + 1)\n                print(f\"Epoch [{epoch+1}/{num_epochs}] | Step [{idx}/{len(train_loader)}] | Loss: {avg_loss:.4f}\")\n\n        \n        if save_model:\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"step\": step,\n            }\n            save_checkpoint(checkpoint, filename=\"/kaggle/working/my_checkpoint.pth.tar\")\n\n        epoch_loss = running_loss / len(train_loader)\n        print(f\"âœ… Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {epoch_loss:.4f}\")\n\n    print(\"ðŸŽ¯ Training completed successfully!\")\n\nif __name__ == \"__main__\":\n    train()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PEfNX6gma8lV","outputId":"2d61a23e-fe27-4ae1-c483-1fffd8d27665","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:19:29.599469Z","iopub.execute_input":"2025-12-19T10:19:29.600131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n\ndef print_examples(model, device, dataset):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    model.eval()\n    test_img1 = transform(Image.open(\"/kaggle/input/testflikr/dog.jpg\").convert(\"RGB\")).unsqueeze(\n        0\n    )\n    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n    print(\n        \"Example 1 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n    )\n    test_img2 = transform(\n        Image.open(\"/kaggle/input/testflikr/child.jpg\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n    print(\n        \"Example 2 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n    )\n    test_img3 = transform(Image.open(\"/kaggle/input/testflikr/bus.png\").convert(\"RGB\")).unsqueeze(\n        0\n    )\n    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n    print(\n        \"Example 3 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n    )\n    test_img4 = transform(\n        Image.open(\"/kaggle/input/testflikr/boat.png\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 4 CORRECT: A small boat in the ocean\")\n    print(\n        \"Example 4 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n    )\n    test_img5 = transform(\n        Image.open(\"/kaggle/input/testflikr/horse.png\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n    print(\n        \"Example 5 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n    )\n    test_img6 = transform(\n        Image.open(\"/kaggle/input/test1flikr/download.jpeg\").convert(\"RGB\")\n    ).unsqueeze(0)\n    \n    print(\n        \"Example 6 OUTPUT: \"\n        + \" \".join(model.caption_image(test_img6.to(device), dataset.vocab))\n    )\n    model.train()","metadata":{"id":"xbcithw-bC23","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ntransform = transforms.Compose([\n    transforms.Resize((356, 356)),\n    transforms.RandomCrop((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_loader, dataset = get_loader(\n    root_folder=\"/kaggle/input/flikr8k/Images\",\n    annotation_file=\"/kaggle/input/flikr8k/captions.txt\",\n    transform=transform,\n    num_workers=2,\n)\n\nembed_size = 256\nhidden_size = 256\nvocab_size = len(dataset.vocab)\nnum_layers = 1\n\nmodel = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n\n\ncheckpoint = torch.load(\"/kaggle/working/my_checkpoint.pth.tar\", map_location=device)\nmodel.load_state_dict(checkpoint[\"state_dict\"])\n\nprint_examples(model, device, dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}